{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import scipy.misc as spm\n",
    "from scipy import ndimage\n",
    "import datetime\n",
    "import matplotlib.image as plt\n",
    "from IPython.display import Image, display\n",
    "from skimage.transform import resize\n",
    "\n",
    "IMG_DIR = r'/home/ubuntu/coding/cnn/datasets/imdb_crop'\n",
    "MAT_FILE = r'/home/ubuntu/coding/cnn/datasets/imdb_crop/imdb.mat'\n",
    "\n",
    "img_depth = 1\n",
    "img_size = 128\n",
    "num_classes = 2\n",
    "\n",
    "max_bytes = 2**31 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the labels, which was not easily obtained. The meta data is stored separately and in a .mat file. (Yes, matlab)!\n",
    "\n",
    "The age parameter, requires us to calculate by taking the ```photo_taken``` and subtracting the ```dob```, the date of birth. Sounds easy? No ... as the dob is stored as a Matlab serial number.\n",
    "\n",
    "Luckily we can use the ```scipy.io.loadmat``` to load the ```.mat``` file to python accessible (kind of) format. We can access the ```dob``` by some proper indexing, and convert the Matlab serial number to a usable format by using ```datetime.date.fromordinal( serial_number ).year```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_date(mat_date):\n",
    "    dt = datetime.date.fromordinal(np.max([mat_date - 366, 1])).year\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path(path):\n",
    "    return os.path.join(IMG_DIR, path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary created...\n"
     ]
    }
   ],
   "source": [
    "mat_struct = sio.loadmat(MAT_FILE)\n",
    "data_set = [data[0] for data in mat_struct['imdb'][0, 0]]\n",
    "\n",
    "keys = ['dob',\n",
    "    'photo_taken',\n",
    "    'full_path',\n",
    "    'gender',\n",
    "    'name',\n",
    "    'face_location',\n",
    "    'face_score',\n",
    "    'second_face_score',\n",
    "    'celeb_names',\n",
    "    'celeb_id'\n",
    "]\n",
    "\n",
    "imdb_dict = dict(zip(keys, np.asarray(data_set)))\n",
    "imdb_dict['dob'] = [reformat_date(dob) for dob in imdb_dict['dob']]\n",
    "imdb_dict['full_path'] = [create_path(path) for path in imdb_dict['full_path']]\n",
    "\n",
    "# Add 'age' key to the dictionary\n",
    "imdb_dict['age'] = imdb_dict['photo_taken'] - imdb_dict['dob']\n",
    "\n",
    "print(\"Dictionary created...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB dataset has total 460,723 face images from 20,284 celebrities. \n",
    "\n",
    "We will ignore:\n",
    "* images with more than one face\n",
    "* gender is NaN\n",
    "* invalid age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 of 460723\n",
      "Second face score: 1.1189733571573068 Age: 69 Gender: 1.0\n",
      "Processing 200000 of 460723\n",
      "Second face score: nan Age: 55 Gender: 1.0\n",
      "Processing 400000 of 460723\n",
      "Second face score: nan Age: 27 Gender: 0.0\n"
     ]
    }
   ],
   "source": [
    "raw_path = imdb_dict['full_path']\n",
    "raw_age = imdb_dict['age']\n",
    "raw_gender = imdb_dict['gender']\n",
    "raw_sface = imdb_dict['second_face_score']\n",
    "\n",
    "age = []\n",
    "gender = []\n",
    "imgs = []\n",
    "for i, sface in enumerate(raw_sface):\n",
    "    if i%200000==0:\n",
    "        print(\"Processing {0} of {1}\".format(i,len(raw_sface)))\n",
    "#         display(Image(filename=raw_path[i]))\n",
    "        print(\"Second face score: {}\".format(sface), end=\" \")\n",
    "        print(\"Age: {}\".format(raw_age[i]), end=\" \")\n",
    "        print(\"Gender: {}\".format(raw_gender[i]))\n",
    "    if np.isnan(sface) and raw_age[i] >= 0 and not np.isnan(raw_gender[i]):\n",
    "        age.append(raw_age[i])\n",
    "        gender.append(raw_gender[i])\n",
    "        imgs.append(raw_path[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some photos are colored and some are gray scale, while the sizes are not consistent. Moreover, processing file in RBG format is too big, when attemp to save objects to pickle file, 60000 file is equivalent to 11GB. So we gonna resize image to 128x128, convert to grayscale.\n",
    "\n",
    "Also due to limit of resources and time, I only pick first ```100000``` images to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already present - Skip convert images to images.\n"
     ]
    }
   ],
   "source": [
    "# Convert images path to images.\n",
    "\n",
    "# only take a subset of dataset: first 10000 imgs\n",
    "# dataset = np.ndarray(shape=(100000, img_size, img_size, img_depth), dtype=np.float32)\n",
    "\n",
    "if os.path.exists(os.getcwd()+\"/pkl_folder/imdb_data_train.pkl\") and os.path.exists(\n",
    "    os.getcwd()+\"/pkl_folder/imdb_data_valid.pkl\") and os.path.exists(\n",
    "    os.getcwd()+\"/pkl_folder/imdb_data_test.pkl\"):\n",
    "    print(\"Dataset already present - Skip convert images to images.\")\n",
    "else:\n",
    "    print(\"Converting images path to images.\")\n",
    "    real_imgs = []\n",
    "    tmp = []\n",
    "    for i, img_path in enumerate(imgs):\n",
    "        if i==100000:\n",
    "            break\n",
    "        tmp = np.asarray(spm.imresize(spm.imread(img_path, flatten=1), (128, 128)), dtype=np.float32)\n",
    "        real_imgs.append(tmp)\n",
    "\n",
    "    print(\"Original size: {0} - Preprocess size: {1}\".format(len(raw_sface), len(real_imgs)))\n",
    "    \n",
    "#     print(\"Converting images path to images.\")\n",
    "#     for i, img_path in enumerate(imgs):\n",
    "#         if i == 100000:\n",
    "#             break\n",
    "#         image_data = resize(((ndimage.imread(img_path).astype(float) - img_depth / 2) / img_depth), \n",
    "#                             (img_size, img_size, img_depth), mode='reflect')\n",
    "#         dataset[i, :, :, :] = image_data\n",
    "\n",
    "#     print(\"Original size: {0} - Preprocess size: {1}\".format(len(raw_sface), len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump 3 datasets to pickles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already present - Skip pickling.\n"
     ]
    }
   ],
   "source": [
    "def dump_data(file_path, slice_from, slice_to):\n",
    "    data = {'image_inputs': np.array(real_imgs[slice_from:slice_to]),\n",
    "            'age_labels': np.array(age[slice_from:slice_to]),\n",
    "            'gender_labels': np.array(gender[slice_from:slice_to])\n",
    "            }\n",
    "    print(\"Dataset dump size: {}\".format(len(data['image_inputs'])))\n",
    "    with open(file_path,'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Dumped to {}\".format(file_path))\n",
    "\n",
    "def create_pickle(force=False):\n",
    "    data_train_path = os.getcwd()+\"/pkl_folder/imdb_data_train.pkl\"\n",
    "    data_valid_path = os.getcwd()+\"/pkl_folder/imdb_data_valid.pkl\"\n",
    "    data_test_path = os.getcwd()+\"/pkl_folder/imdb_data_test.pkl\"\n",
    "    if os.path.exists(data_train_path) and os.path.exists(\n",
    "        data_valid_path) and os.path.exists(\n",
    "        data_test_path) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print(\"Dataset already present - Skip pickling.\")\n",
    "        \n",
    "    else:\n",
    "        dump_data(data_train_path, 0, 60000)\n",
    "        dump_data(data_valid_path, 60000, 80000)\n",
    "        dump_data(data_test_path, 80000, 1000000)\n",
    "\n",
    "    return data_train_path, data_valid_path, data_test_path\n",
    "\n",
    "data_train_path, data_valid_path, data_test_path = create_pickle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using only a subset of the data, and also using a self-constructed model that has a much smaller capacity, thus we need to take steps to adjust accordingly.\n",
    "\n",
    "The original paper uses 101101 age classes, which was appropriate for the their data set size and learning architecture. As we are only using a small subset of the data and a very simple model, the number of classes was set to 4:\n",
    "* Young (30yrs < age)\n",
    "* Middle (30 <= age <45)\n",
    "* Old (45 <= age < 60)\n",
    "* Very Old ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (60000, 128, 128) (60000, 2)\n",
      "Validation: (20000, 128, 128) (20000, 2)\n",
      "Testing: (20000, 128, 128) (20000, 2)\n"
     ]
    }
   ],
   "source": [
    "def convert_label(pickle_file):\n",
    "    try:\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            data_train = pickle.load(f)\n",
    "            labels = np.ndarray((len(data_train['image_inputs']), num_classes), dtype=np.int32)\n",
    "            dataset = np.ndarray((len(data_train['image_inputs']), img_size, img_size, img_depth), dtype=np.float32)\n",
    "            # let's shuffle to have random dataset\n",
    "            np.random.shuffle(dataset)\n",
    "            dataset = data_train['image_inputs']\n",
    "            for i, age_label in enumerate(data_train['age_labels']):\n",
    "                if i==len(data_train['image_inputs']):\n",
    "                    break\n",
    "                if age_label < 30:\n",
    "                    age = 0\n",
    "                elif age_label <= 45:\n",
    "                    age = 1\n",
    "                elif age_label < 60:\n",
    "                    age = 2\n",
    "                elif age_label >= 60:\n",
    "                    age = 3\n",
    "                else:\n",
    "                    continue\n",
    "                labels[i,:] = np.array([age, data_train['gender_labels'][i]])\n",
    "            return dataset, labels\n",
    "            \n",
    "    except Exception as e:\n",
    "        print('Unable to process data from', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "train_dataset, train_labels = convert_label(data_train_path)\n",
    "valid_dataset, valid_labels = convert_label(data_valid_path)\n",
    "test_dataset, test_labels = convert_label(data_test_path)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll randomize the data. It's important to have the labels well shuffled for the training and test distributions to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "* convolutions need the image data formatted as a cube (width by height by channels)\n",
    "* labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (60000, 128, 128, 1) (60000, 2, 4)\n",
      "Validation set (20000, 128, 128, 1) (20000, 2, 4)\n",
      "Test set (20000, 128, 128, 1) (20000, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "num_channels = img_depth # = 1 (Grayscale)\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, img_size, img_size, num_channels)).astype(np.float32)\n",
    "    one_hot_labels = np.ndarray((len(labels), 2, 4), dtype=np.float32)\n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot_age = (np.arange(4)==label[0]).astype(np.float32)\n",
    "        one_hot_gender = (np.arange(4)==label[1]).astype(np.float32)\n",
    "        one_hot_labels[i,:,:] = np.array([one_hot_age, one_hot_gender])\n",
    "#     labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, one_hot_labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to final pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 6556800496\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'imdb.pkl'\n",
    "\n",
    "try:\n",
    "    f = open(os.getcwd()+\"/pkl_folder/\"+pickle_file, 'wb')\n",
    "    save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise\n",
    "\n",
    "statinfo = os.stat(os.getcwd()+\"/pkl_folder/\"+pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
